{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_oYHNNyPp4v",
        "outputId": "d2e22864-252e-4456-b3a7-a7e3db1c5732"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.12.4)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.13.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras) (24.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install keras scikit-learn imbalanced-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from sklearn.datasets import fetch_lfw_people\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from imblearn.over_sampling import RandomOverSampler"
      ],
      "metadata": {
        "id": "J2NcosP4P5zh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lfw_people = fetch_lfw_people(min_faces_per_person=20, color=True, resize=1.0,\n",
        "                              slice_=(slice(48, 202), slice(48, 202)))\n",
        "\n",
        "X = lfw_people.images\n",
        "y = lfw_people.target\n",
        "target_names = lfw_people.target_names\n",
        "n_classes = target_names.shape[0]\n",
        "\n",
        "print('number of examples: {}'.format(y.shape[0]))\n",
        "print('dimensionality of images: {}'.format(X.shape[1:]))\n",
        "print('number of unique classes (people): {}'.format(n_classes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGdat-EWQFly",
        "outputId": "38b199be-6bc0-40a6-ab91-ce044b994679"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of examples: 3023\n",
            "dimensionality of images: (154, 154, 3)\n",
            "number of unique classes (people): 62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (target_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxskZXZVQkJc",
        "outputId": "31656c6f-fe9a-4d24-fc7a-457dc2fa4831"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Alejandro Toledo' 'Alvaro Uribe' 'Amelie Mauresmo' 'Andre Agassi'\n",
            " 'Angelina Jolie' 'Ariel Sharon' 'Arnold Schwarzenegger'\n",
            " 'Atal Bihari Vajpayee' 'Bill Clinton' 'Carlos Menem' 'Colin Powell'\n",
            " 'David Beckham' 'Donald Rumsfeld' 'George Robertson' 'George W Bush'\n",
            " 'Gerhard Schroeder' 'Gloria Macapagal Arroyo' 'Gray Davis'\n",
            " 'Guillermo Coria' 'Hamid Karzai' 'Hans Blix' 'Hugo Chavez' 'Igor Ivanov'\n",
            " 'Jack Straw' 'Jacques Chirac' 'Jean Chretien' 'Jennifer Aniston'\n",
            " 'Jennifer Capriati' 'Jennifer Lopez' 'Jeremy Greenstock' 'Jiang Zemin'\n",
            " 'John Ashcroft' 'John Negroponte' 'Jose Maria Aznar'\n",
            " 'Juan Carlos Ferrero' 'Junichiro Koizumi' 'Kofi Annan' 'Laura Bush'\n",
            " 'Lindsay Davenport' 'Lleyton Hewitt' 'Luiz Inacio Lula da Silva'\n",
            " 'Mahmoud Abbas' 'Megawati Sukarnoputri' 'Michael Bloomberg' 'Naomi Watts'\n",
            " 'Nestor Kirchner' 'Paul Bremer' 'Pete Sampras' 'Recep Tayyip Erdogan'\n",
            " 'Ricardo Lagos' 'Roh Moo-hyun' 'Rudolph Giuliani' 'Saddam Hussein'\n",
            " 'Serena Williams' 'Silvio Berlusconi' 'Tiger Woods' 'Tom Daschle'\n",
            " 'Tom Ridge' 'Tony Blair' 'Vicente Fox' 'Vladimir Putin' 'Winona Ryder']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_ = plt.hist(y, bins=n_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "gVVWMrcURFSW",
        "outputId": "76706a66-2c8c-496b-ff27-a40931c72f84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = X / 255.0\n"
      ],
      "metadata": {
        "id": "-T_EkkB5OXBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "8N-xfYNSOYZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from sklearn.datasets import fetch_lfw_people\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load LFW dataset (you've already done this part)\n",
        "lfw_people = fetch_lfw_people(min_faces_per_person=20, color=True, resize=1.0)\n",
        "X = lfw_people.images\n",
        "y = lfw_people.target\n",
        "target_names = lfw_people.target_names\n",
        "n_classes = target_names.shape[0]\n",
        "\n",
        "# Reshape the images to match the input shape expected by the model (154x154x3)\n",
        "X = np.array([np.resize(image, (154, 154, 3)) for image in X])  # Resize images to (154, 154, 3)\n",
        "\n",
        "# Normalize images to be between 0 and 1\n",
        "X = X.astype('float32') / 255.0\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert labels to categorical\n",
        "y_train = to_categorical(y_train, num_classes=n_classes)\n",
        "y_test = to_categorical(y_test, num_classes=n_classes)\n",
        "\n",
        "# Define CNN Model\n",
        "face_recognition_model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(154, 154, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "\n",
        "    Dense(n_classes, activation='softmax')  # Output layer for n_classes (15 in this case)\n",
        "])\n",
        "\n",
        "# Compile the Model\n",
        "face_recognition_model.compile(optimizer='adam',\n",
        "                                loss='categorical_crossentropy',  # Use categorical_crossentropy for multi-class classification\n",
        "                                metrics=['accuracy'])\n",
        "\n",
        "# Train the Model\n",
        "face_recognition_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_loss, test_acc = face_recognition_model.evaluate(X_test, y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KeDECatObrk",
        "outputId": "e5c6a708-1d83-4907-bdf8-d7185c337b2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 154ms/step - accuracy: 0.0901 - loss: 6.4333 - val_accuracy: 0.0149 - val_loss: 4.0264\n",
            "Epoch 2/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 47ms/step - accuracy: 0.2233 - loss: 3.4320 - val_accuracy: 0.0975 - val_loss: 3.9233\n",
            "Epoch 3/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.3301 - loss: 2.8916 - val_accuracy: 0.0975 - val_loss: 3.9258\n",
            "Epoch 4/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.4388 - loss: 2.2504 - val_accuracy: 0.0893 - val_loss: 4.0003\n",
            "Epoch 5/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.5591 - loss: 1.6590 - val_accuracy: 0.0975 - val_loss: 4.3262\n",
            "Epoch 6/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6721 - loss: 1.1827 - val_accuracy: 0.0215 - val_loss: 4.9479\n",
            "Epoch 7/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7507 - loss: 0.9085 - val_accuracy: 0.1603 - val_loss: 4.7753\n",
            "Epoch 8/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.7537 - loss: 0.8250 - val_accuracy: 0.0264 - val_loss: 6.6723\n",
            "Epoch 9/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.8308 - loss: 0.5569 - val_accuracy: 0.2000 - val_loss: 5.2461\n",
            "Epoch 10/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.8598 - loss: 0.4982 - val_accuracy: 0.0397 - val_loss: 19.4896\n",
            "Epoch 11/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - accuracy: 0.8500 - loss: 0.5041 - val_accuracy: 0.0430 - val_loss: 118.7510\n",
            "Epoch 12/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.8765 - loss: 0.4074 - val_accuracy: 0.0975 - val_loss: 25.5741\n",
            "Epoch 13/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.8840 - loss: 0.3981 - val_accuracy: 0.0975 - val_loss: 64.0259\n",
            "Epoch 14/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.8725 - loss: 0.4378 - val_accuracy: 0.1802 - val_loss: 19.0132\n",
            "Epoch 15/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.8977 - loss: 0.3479 - val_accuracy: 0.3669 - val_loss: 5.1759\n",
            "Epoch 16/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.9025 - loss: 0.3110 - val_accuracy: 0.0050 - val_loss: 299.6320\n",
            "Epoch 17/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8967 - loss: 0.3351 - val_accuracy: 0.2926 - val_loss: 12.4167\n",
            "Epoch 18/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8982 - loss: 0.3831 - val_accuracy: 0.0562 - val_loss: 46.1258\n",
            "Epoch 19/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.9246 - loss: 0.2864 - val_accuracy: 0.0149 - val_loss: 176.2821\n",
            "Epoch 20/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.9321 - loss: 0.2388 - val_accuracy: 0.0215 - val_loss: 155.0988\n",
            "Epoch 21/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.9255 - loss: 0.2491 - val_accuracy: 0.0397 - val_loss: 77.5623\n",
            "Epoch 22/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.9327 - loss: 0.2272 - val_accuracy: 0.0066 - val_loss: 651.4581\n",
            "Epoch 23/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9280 - loss: 0.2727 - val_accuracy: 0.0215 - val_loss: 246.9154\n",
            "Epoch 24/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.9320 - loss: 0.2468 - val_accuracy: 0.0380 - val_loss: 72.2808\n",
            "Epoch 25/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.9307 - loss: 0.2477 - val_accuracy: 0.2793 - val_loss: 18.3755\n",
            "Epoch 26/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.9155 - loss: 0.3079 - val_accuracy: 0.0050 - val_loss: 139.5960\n",
            "Epoch 27/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.9165 - loss: 0.3171 - val_accuracy: 0.1521 - val_loss: 49.1324\n",
            "Epoch 28/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.9421 - loss: 0.1829 - val_accuracy: 0.0926 - val_loss: 115.1034\n",
            "Epoch 29/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.9344 - loss: 0.2463 - val_accuracy: 0.0231 - val_loss: 164.9927\n",
            "Epoch 30/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.9403 - loss: 0.2053 - val_accuracy: 0.0314 - val_loss: 92.1812\n",
            "Epoch 31/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.9467 - loss: 0.1955 - val_accuracy: 0.0264 - val_loss: 130.3415\n",
            "Epoch 32/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.9464 - loss: 0.1941 - val_accuracy: 0.0364 - val_loss: 608.0408\n",
            "Epoch 33/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.9486 - loss: 0.1961 - val_accuracy: 0.0562 - val_loss: 218.7953\n",
            "Epoch 34/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.9573 - loss: 0.1582 - val_accuracy: 0.0182 - val_loss: 317.4031\n",
            "Epoch 35/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.9499 - loss: 0.2065 - val_accuracy: 0.0397 - val_loss: 110.3956\n",
            "Epoch 36/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.9430 - loss: 0.1970 - val_accuracy: 0.1636 - val_loss: 89.0262\n",
            "Epoch 37/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.9546 - loss: 0.2035 - val_accuracy: 0.0909 - val_loss: 171.4619\n",
            "Epoch 38/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.9334 - loss: 0.2978 - val_accuracy: 0.4248 - val_loss: 8.9538\n",
            "Epoch 39/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.9418 - loss: 0.2499 - val_accuracy: 0.0298 - val_loss: 354.7481\n",
            "Epoch 40/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.9469 - loss: 0.1785 - val_accuracy: 0.0050 - val_loss: 308.6100\n",
            "Epoch 41/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.9463 - loss: 0.2159 - val_accuracy: 0.0281 - val_loss: 268.5955\n",
            "Epoch 42/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.9461 - loss: 0.2085 - val_accuracy: 0.0281 - val_loss: 125.7931\n",
            "Epoch 43/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.9371 - loss: 0.2598 - val_accuracy: 0.0628 - val_loss: 231.0925\n",
            "Epoch 44/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.9453 - loss: 0.2020 - val_accuracy: 0.0760 - val_loss: 497.0925\n",
            "Epoch 45/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.9459 - loss: 0.2019 - val_accuracy: 0.0298 - val_loss: 1012.8714\n",
            "Epoch 46/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.9538 - loss: 0.2243 - val_accuracy: 0.0116 - val_loss: 920.2566\n",
            "Epoch 47/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.9560 - loss: 0.1847 - val_accuracy: 0.0165 - val_loss: 662.9802\n",
            "Epoch 48/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.9613 - loss: 0.1544 - val_accuracy: 0.0380 - val_loss: 375.0153\n",
            "Epoch 49/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.9357 - loss: 0.2476 - val_accuracy: 0.0694 - val_loss: 344.1597\n",
            "Epoch 50/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.9460 - loss: 0.2246 - val_accuracy: 0.0298 - val_loss: 639.4375\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0289 - loss: 624.7101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Model\n",
        "face_recognition_model.save('face_recognition_lfw.h5')\n",
        "\n",
        "# Load Model\n",
        "from tensorflow.keras.models import load_model\n",
        "model = load_model('face_recognition_lfw.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCV0tujpPkOU",
        "outputId": "d70544ff-b900-4dc6-8119-882169428b8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('face_recognition_model.keras')\n"
      ],
      "metadata": {
        "id": "TieO8As9PxC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = '/content/archive.zip'  # Path to the zip file\n",
        "extract_to = '/content/fer_dataset'  # Extraction directory\n",
        "\n",
        "# Extract the dataset\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to)\n",
        "\n",
        "print(\"Dataset extracted!\")\n",
        "\n",
        "# Correcting the train and test directories\n",
        "train_dir = os.path.join(extract_to, 'train')\n",
        "test_dir = os.path.join(extract_to, 'test')\n",
        "\n",
        "# Verify directory structure\n",
        "print(\"Train directory contents:\", os.listdir(train_dir))\n",
        "print(\"Test directory contents:\", os.listdir(test_dir))\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Image parameters\n",
        "img_width, img_height = 48, 48\n",
        "batch_size = 32\n",
        "num_classes = 7  # Number of emotions: angry, disgust, fear, happy, neutral, sad, surprise\n",
        "\n",
        "# Prepare data generators with data augmentation for better generalization\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,  # Normalize the image pixels\n",
        "    rotation_range=30,  # Randomly rotate images\n",
        "    width_shift_range=0.2,  # Randomly shift images horizontally\n",
        "    height_shift_range=0.2,  # Randomly shift images vertically\n",
        "    shear_range=0.2,  # Shear images for data augmentation\n",
        "    zoom_range=0.2,  # Zoom in or out\n",
        "    horizontal_flip=True,  # Flip images horizontally\n",
        "    fill_mode='nearest'  # Fill missing pixels during transformation\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)  # Only rescaling for test data\n",
        "\n",
        "# Load the data from the corrected paths\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Define CNN model for emotion detection\n",
        "emotion_detection_model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(num_classes, activation='softmax')  # Output layer for emotion classes (7 emotions)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "emotion_detection_model.compile(optimizer='adam',\n",
        "                                loss='categorical_crossentropy',  # Using categorical crossentropy for multi-class\n",
        "                                metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "emotion_detection_model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // batch_size,\n",
        "    epochs=50,\n",
        "    validation_data=test_generator,\n",
        "    validation_steps=test_generator.samples // batch_size\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = emotion_detection_model.evaluate(test_generator)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djsjcANGRYQe",
        "outputId": "5fdb9f1a-34e6-4025-e363-8562257efb10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset extracted!\n",
            "Train directory contents: ['fear', 'disgust', 'neutral', 'sad', 'surprise', 'happy', 'angry']\n",
            "Test directory contents: ['fear', 'disgust', 'neutral', 'sad', 'surprise', 'happy', 'angry']\n",
            "Found 28709 images belonging to 7 classes.\n",
            "Found 7178 images belonging to 7 classes.\n",
            "Epoch 1/50\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 52ms/step - accuracy: 0.2167 - loss: 2.5747 - val_accuracy: 0.2796 - val_loss: 1.7723\n",
            "Epoch 2/50\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 384us/step - accuracy: 0.0938 - loss: 1.7860 - val_accuracy: 0.2000 - val_loss: 2.1959\n",
            "Epoch 3/50\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 48ms/step - accuracy: 0.2856 - loss: 1.7589 - val_accuracy: 0.3432 - val_loss: 1.6503\n",
            "Epoch 4/50\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17us/step - accuracy: 0.3438 - loss: 1.6714 - val_accuracy: 0.6000 - val_loss: 1.3987\n",
            "Epoch 5/50\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 47ms/step - accuracy: 0.3156 - loss: 1.6903 - val_accuracy: 0.3610 - val_loss: 1.5910\n",
            "Epoch 6/50\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15us/step - accuracy: 0.4688 - loss: 1.2961 - val_accuracy: 0.4000 - val_loss: 1.5049\n",
            "Epoch 7/50\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 50ms/step - accuracy: 0.3535 - loss: 1.6373 - val_accuracy: 0.4042 - val_loss: 1.5128\n",
            "Epoch 8/50\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16us/step - accuracy: 0.4688 - loss: 1.7180 - val_accuracy: 0.4000 - val_loss: 1.6759\n",
            "Epoch 9/50\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 48ms/step - accuracy: 0.3786 - loss: 1.5822 - val_accuracy: 0.4703 - val_loss: 1.3977\n",
            "Epoch 10/50\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16us/step - accuracy: 0.2188 - loss: 1.6749 - val_accuracy: 0.7000 - val_loss: 1.0021\n",
            "Epoch 11/50\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 50ms/step - accuracy: 0.4056 - loss: 1.5289 - val_accuracy: 0.4240 - val_loss: 1.4736\n",
            "Epoch 12/50\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22us/step - accuracy: 0.2812 - loss: 1.7970 - val_accuracy: 0.3000 - val_loss: 1.4936\n",
            "Epoch 13/50\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 49ms/step - accuracy: 0.4179 - loss: 1.4983 - val_accuracy: 0.4037 - val_loss: 1.5829\n",
            "Epoch 14/50\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240us/step - accuracy: 0.4375 - loss: 1.2828 - val_accuracy: 0.4000 - val_loss: 1.4805\n",
            "Epoch 15/50\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 51ms/step - accuracy: 0.4323 - loss: 1.4696 - val_accuracy: 0.4946 - val_loss: 1.3203\n",
            "Epoch 16/50\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.4688 - loss: 1.3560 - val_accuracy: 0.8000 - val_loss: 0.7487\n",
            "Epoch 17/50\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 49ms/step - accuracy: 0.4467 - loss: 1.4404 - val_accuracy: 0.4728 - val_loss: 1.3231\n",
            "Epoch 18/50\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17us/step - accuracy: 0.4375 - loss: 1.4767 - val_accuracy: 0.3000 - val_loss: 1.4598\n",
            "Epoch 19/50\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 50ms/step - accuracy: 0.4520 - loss: 1.4281 - val_accuracy: 0.4929 - val_loss: 1.3222\n",
            "Epoch 20/50\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17us/step - accuracy: 0.4688 - loss: 1.2949 - val_accuracy: 0.6000 - val_loss: 1.3004\n",
            "Epoch 21/50\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 49ms/step - accuracy: 0.4573 - loss: 1.4000 - val_accuracy: 0.4650 - val_loss: 1.4829\n",
            "Epoch 22/50\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171us/step - accuracy: 0.5000 - loss: 1.4952 - val_accuracy: 0.5000 - val_loss: 1.1891\n",
            "Epoch 23/50\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 50ms/step - accuracy: 0.4674 - loss: 1.3866 - val_accuracy: 0.5370 - val_loss: 1.2261\n",
            "Epoch 24/50\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21us/step - accuracy: 0.5312 - loss: 1.1903 - val_accuracy: 0.6000 - val_loss: 0.8825\n",
            "Epoch 25/50\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 51ms/step - accuracy: 0.4749 - loss: 1.3702 - val_accuracy: 0.4884 - val_loss: 1.3237\n",
            "Epoch 26/50\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17us/step - accuracy: 0.5000 - loss: 1.2760 - val_accuracy: 0.5000 - val_loss: 0.9798\n",
            "Epoch 27/50\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 49ms/step - accuracy: 0.4803 - loss: 1.3533 - val_accuracy: 0.3456 - val_loss: 1.6659\n",
            "Epoch 28/50\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15us/step - accuracy: 0.4375 - loss: 1.4230 - val_accuracy: 0.4000 - val_loss: 1.3901\n",
            "Epoch 29/50\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 49ms/step - accuracy: 0.4872 - loss: 1.3440 - val_accuracy: 0.5386 - val_loss: 1.2149\n",
            "Epoch 30/50\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16us/step - accuracy: 0.5625 - loss: 1.1021 - val_accuracy: 0.7000 - val_loss: 1.0884\n",
            "Epoch 31/50\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 49ms/step - accuracy: 0.4951 - loss: 1.3211 - val_accuracy: 0.5198 - val_loss: 1.2633\n",
            "Epoch 32/50\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202us/step - accuracy: 0.6250 - loss: 0.9537 - val_accuracy: 0.5000 - val_loss: 1.1618\n",
            "Epoch 33/50\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 48ms/step - accuracy: 0.4975 - loss: 1.3222 - val_accuracy: 0.5296 - val_loss: 1.2333\n",
            "Epoch 34/50\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244us/step - accuracy: 0.4688 - loss: 1.4035 - val_accuracy: 0.3000 - val_loss: 1.7489\n",
            "Epoch 35/50\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 49ms/step - accuracy: 0.4946 - loss: 1.3159 - val_accuracy: 0.5525 - val_loss: 1.1915\n",
            "Epoch 36/50\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21us/step - accuracy: 0.4062 - loss: 1.3573 - val_accuracy: 0.7000 - val_loss: 0.8274\n",
            "Epoch 37/50\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 48ms/step - accuracy: 0.5134 - loss: 1.2927 - val_accuracy: 0.5414 - val_loss: 1.2315\n",
            "Epoch 38/50\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213us/step - accuracy: 0.4062 - loss: 1.2496 - val_accuracy: 0.2000 - val_loss: 1.8867\n",
            "Epoch 39/50\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 50ms/step - accuracy: 0.5121 - loss: 1.2978 - val_accuracy: 0.5578 - val_loss: 1.1808\n",
            "Epoch 40/50\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.5000 - loss: 1.2098 - val_accuracy: 0.5000 - val_loss: 1.1499\n",
            "Epoch 41/50\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 48ms/step - accuracy: 0.5055 - loss: 1.2912 - val_accuracy: 0.5459 - val_loss: 1.2066\n",
            "Epoch 42/50\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15us/step - accuracy: 0.3750 - loss: 1.5935 - val_accuracy: 0.8000 - val_loss: 0.7258\n",
            "Epoch 43/50\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 48ms/step - accuracy: 0.5115 - loss: 1.2934 - val_accuracy: 0.5412 - val_loss: 1.2078\n",
            "Epoch 44/50\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16us/step - accuracy: 0.7188 - loss: 0.9922 - val_accuracy: 0.8000 - val_loss: 0.7570\n",
            "Epoch 45/50\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 48ms/step - accuracy: 0.5221 - loss: 1.2663 - val_accuracy: 0.5763 - val_loss: 1.1350\n",
            "Epoch 46/50\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5312 - loss: 1.3686 - val_accuracy: 0.5000 - val_loss: 1.2529\n",
            "Epoch 47/50\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 48ms/step - accuracy: 0.5246 - loss: 1.2656 - val_accuracy: 0.5727 - val_loss: 1.1348\n",
            "Epoch 48/50\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17us/step - accuracy: 0.4688 - loss: 1.2127 - val_accuracy: 0.4000 - val_loss: 1.2095\n",
            "Epoch 49/50\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 48ms/step - accuracy: 0.5247 - loss: 1.2575 - val_accuracy: 0.5575 - val_loss: 1.1699\n",
            "Epoch 50/50\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22us/step - accuracy: 0.5312 - loss: 1.2246 - val_accuracy: 0.6000 - val_loss: 0.9881\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.5411 - loss: 1.2095\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emotion_model.save('emotion_detection_model.keras')\n"
      ],
      "metadata": {
        "id": "qHL1ztzrWFel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Load Models\n",
        "face_recognition_model = load_model('face_recognition_model.keras')\n",
        "emotion_detection_model = load_model('emotion_detection_model.keras')\n",
        "\n",
        "# Class Labels\n",
        "face_labels = [\n",
        "    'Alejandro Toledo', 'Alvaro Uribe', 'Amelie Mauresmo', 'Andre Agassi',\n",
        "    'Angelina Jolie', 'Ariel Sharon', 'Arnold Schwarzenegger',\n",
        "    'Atal Bihari Vajpayee', 'Bill Clinton', 'Carlos Menem', 'Colin Powell',\n",
        "    'David Beckham', 'Donald Rumsfeld', 'George Robertson', 'George W Bush',\n",
        "    'Gerhard Schroeder', 'Gloria Macapagal Arroyo', 'Gray Davis',\n",
        "    'Guillermo Coria', 'Hamid Karzai', 'Hans Blix', 'Hugo Chavez', 'Igor Ivanov',\n",
        "    'Jack Straw', 'Jacques Chirac', 'Jean Chretien', 'Jennifer Aniston',\n",
        "    'Jennifer Capriati', 'Jennifer Lopez', 'Jeremy Greenstock', 'Jiang Zemin',\n",
        "    'John Ashcroft', 'John Negroponte', 'Jose Maria Aznar',\n",
        "    'Juan Carlos Ferrero', 'Junichiro Koizumi', 'Kofi Annan', 'Laura Bush',\n",
        "    'Lindsay Davenport', 'Lleyton Hewitt', 'Luiz Inacio Lula da Silva',\n",
        "    'Mahmoud Abbas', 'Megawati Sukarnoputri', 'Michael Bloomberg', 'Naomi Watts',\n",
        "    'Nestor Kirchner', 'Paul Bremer', 'Pete Sampras', 'Recep Tayyip Erdogan',\n",
        "    'Ricardo Lagos', 'Roh Moo-hyun', 'Rudolph Giuliani', 'Saddam Hussein',\n",
        "    'Serena Williams', 'Silvio Berlusconi', 'Tiger Woods', 'Tom Daschle',\n",
        "    'Tom Ridge', 'Tony Blair', 'Vicente Fox', 'Vladimir Putin', 'Winona Ryder'\n",
        "]\n",
        "\n",
        "emotion_labels = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TM24Su02Wuz2",
        "outputId": "42fa3149-a542-4037-c12a-8e1ee63fd40b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'adam', because it has 34 variables whereas the saved optimizer has 2 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Attendance and Emotion Data\n",
        "attendance_data = {name: 'Absent' for name in face_labels}\n",
        "emotions_data = {name: 'N/A' for name in face_labels}\n",
        "\n",
        "# CSV File for Saving Data\n",
        "output_file = 'attendance_emotions.csv'\n"
      ],
      "metadata": {
        "id": "XfSur0HVXHoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Time Window\n",
        "start_time = datetime.strptime(\"09:30:00\", \"%H:%M:%S\").time()\n",
        "end_time = datetime.strptime(\"10:00:00\", \"%H:%M:%S\").time()\n",
        "\n",
        "# Start Video Capture\n",
        "cap = cv2.VideoCapture(0)  # Replace `0` with video file path if not using webcam\n",
        "\n",
        "while True:\n",
        "    current_time = datetime.now().time()\n",
        "    if current_time < start_time or current_time > end_time:\n",
        "        print(\"Outside of allowed time window. Exiting.\")\n",
        "        break\n",
        "\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Convert to Grayscale for Face Detection\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Detect Faces\n",
        "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
        "\n",
        "    for (x, y, w, h) in faces:\n",
        "        # Draw Rectangle Around Face\n",
        "        cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
        "\n",
        "        # Preprocess Face for Recognition\n",
        "        face = frame[y:y + h, x:x + w]\n",
        "        face_rgb = cv2.resize(face, (154, 154))  # Match size of recognition model\n",
        "        face_rgb = np.expand_dims(face_rgb / 255.0, axis=0)  # Normalize and add batch dimension\n",
        "\n",
        "        # Predict Face\n",
        "        face_pred = face_recognition_model.predict(face_rgb)\n",
        "        face_label = face_labels[np.argmax(face_pred)]\n",
        "\n",
        "        # Preprocess Face for Emotion Detection\n",
        "        face_gray = cv2.resize(cv2.cvtColor(face, cv2.COLOR_BGR2GRAY), (48, 48))\n",
        "        face_gray = np.expand_dims(face_gray / 255.0, axis=(0, -1))  # Normalize and add dimensions\n",
        "\n",
        "        # Predict Emotion\n",
        "        emotion_pred = emotion_detection_model.predict(face_gray)\n",
        "        emotion_label = emotion_labels[np.argmax(emotion_pred)]\n",
        "\n",
        "        # Update Attendance and Emotion Data\n",
        "        attendance_data[face_label] = 'Present'\n",
        "        emotions_data[face_label] = emotion_label\n",
        "\n",
        "        # Annotate Frame\n",
        "        cv2.putText(frame, f'{face_label} - {emotion_label}', (x, y - 10),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
        "\n",
        "    # Display Frame\n",
        "    cv2.imshow('Attendance System', frame)\n",
        "\n",
        "    # Break on 'q' Key Press\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# Release Resources\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Af4wEVeAXOZi",
        "outputId": "5332fc16-434b-427f-a078-ea8b556603ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outside of allowed time window. Exiting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine Data\n",
        "final_data = [{'Name': name, 'Attendance': attendance_data[name], 'Emotion': emotions_data[name], 'Time': datetime.now().strftime(\"%H:%M:%S\")} for name in face_labels]\n",
        "\n",
        "# Save to CSV\n",
        "df = pd.DataFrame(final_data)\n",
        "df.to_csv(output_file, index=False)\n",
        "print(f\"Data saved to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cL_pgctTXUeT",
        "outputId": "be1bd13c-4f52-4d8d-e8d3-f5b5ad784f5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to attendance_emotions.csv\n"
          ]
        }
      ]
    }
  ]
}